{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load in the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "training_data = pd.read_parquet('./training.parquet')\n",
    "training_data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to transform each of the messages into a numeric vector. One way to do this would be to associate a space in a bit vector with every possible word. We could set each bit to `True` or `1` if the corresponding word appears in that message, and `0` or `False` otherwise. In practice this is not practical because of the huge memory requirements. Instead we use a 'hashing vectoriser' which _hashes_ the words to vectors of a fixed, finite number of bits. \n",
    "\n",
    "You can see that many of the messages contain non-standard words or character strings such as email addresses. It is common practice to ignore such words during the feature engineering stage. We do this by setting a 'token_pattern' parameter to ensure that only standard words are hashed. \n",
    "\n",
    "We also want to remove any 'stop words' from the text. We do this by setting `stop_words='english'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "BUCKETS=2048\n",
    "\n",
    "hv = HashingVectorizer(norm=None, token_pattern='(?u)\\\\b[A-Za-z]\\\\w+\\\\b', stop_words ='english', n_features=BUCKETS, alternate_sign = False)\n",
    "hv\n",
    "hvcounts = hv.fit_transform(training_data[\"Text\"])\n",
    "hvcounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use term frequency-inverse document frequency, or tf-idf, to generate feature vectors. Tf-idf is commonly used to summarise text data, and it aims to capture how important different words are within a set of documents. Tf-idf combines a normalized word count (or term frequency) with the inverse document frequency (or a measure of how common a word is across all documents) in order to identify words, or terms, which are 'interesting' within the document, with respect to the set of documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "hvdf_tfidf = tfidf_transformer.fit_transform(hvcounts)\n",
    "hvdf_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.decomposition\n",
    "\n",
    "DIMENSIONS = 2\n",
    "\n",
    "pca2 = sklearn.decomposition.TruncatedSVD(DIMENSIONS)\n",
    "pca_a = pca2.fit_transform(hvdf_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df = pd.DataFrame(pca_a, columns=[\"x\", \"y\"])\n",
    "pca_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = pd.concat([training_data.reset_index(), pca_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "alt.renderers.enable('notebook')\n",
    "alt.Chart(plot_data.sample(2000)).encode(x=\"x\", y=\"y\", color=\"Category\").mark_point().interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No structure. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "from altair.expr import datum\n",
    "\n",
    "data=plot_data.sample(2000)\n",
    "\n",
    "base = alt.Chart(data).mark_point().encode(\n",
    "    alt.X('x', scale=alt.Scale(domain=(0, 0.6))),\n",
    "    alt.Y('y', scale = alt.Scale(domain = (-0.3, 0.3))),\n",
    "    color='Category:N'\n",
    ")\n",
    "\n",
    "## Make gray points for background\n",
    "chart1 = alt.Chart(data.sample(1000)) \\\n",
    "            .encode(x=\"x\", y=\"y\") \\\n",
    "            .mark_point(opacity=0.3, color=\"lightgray\")\n",
    "\n",
    "\n",
    "def overlay_make_chart(base_chart, category, options, chart1):\n",
    "    title = category\n",
    "    chart2 = base_chart\\\n",
    "      .transform_filter(datum.Category == category)\\\n",
    "      .properties(width=options['width'], \n",
    "                    height=options['height'], title=title)\n",
    "    chart = chart1 + chart2\n",
    "    return chart\n",
    "\n",
    "\n",
    "options = {'width': 150, 'height': 150}\n",
    "charts = [overlay_make_chart(base, category, options, chart1) \\\n",
    "          for category in sorted(data.Category.unique())]\n",
    "\n",
    "# make a single row\n",
    "def make_chart_row(row_of_charts):\n",
    "    hconcat = [chart for chart in row_of_charts]\n",
    "    row = alt.HConcatChart(hconcat=hconcat)\n",
    "    return row\n",
    "\n",
    "# take an array of charts and produce a facet grid\n",
    "def facet_wrap(charts, charts_per_row):\n",
    "    rows_of_charts = [\n",
    "        charts[i:i+charts_per_row] \n",
    "        for i in range(0, len(charts), charts_per_row)]        \n",
    "    vconcat = [make_chart_row(r) for r in rows_of_charts]    \n",
    "    vcc = alt.VConcatChart(vconcat=vconcat)\\\n",
    "      .configure_axisX(grid=True)\\\n",
    "      .configure_axisY(grid=True)\n",
    "    return vcc\n",
    "\n",
    "# assemble the facet grid\n",
    "compound_chart = facet_wrap(charts, charts_per_row=3)\n",
    "compound_chart.properties(title='PCA projections, by Category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##okay good - looks better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline \n",
    "\n",
    "BUCKETS=2048\n",
    "hv = HashingVectorizer(norm=None, token_pattern='(?u)\\\\b[A-Za-z]\\\\w+\\\\b', stop_words ='english', n_features=BUCKETS, alternate_sign = False)\n",
    "tfidf = TfidfTransformer()\n",
    "feat_pipeline = Pipeline([('vect',hv), ('tfidf',tfidf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_vecs = feat_pipeline.fit_transform(training_data[\"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "XGB_TREE_METHOD='hist'\n",
    "xgb = XGBClassifier(tree_method=XGB_TREE_METHOD, \n",
    "                    # num_parallel_tree=16, \n",
    "                    n_estimators=100, \n",
    "                    max_depth=3, \n",
    "                    colsample_bynode=0.3, \n",
    "                    colsample_bytree=0.3, \n",
    "                    subsample=1, \n",
    "                    reg_alpha=1)\n",
    "\n",
    "xgb.fit(training_vecs, training_data[\"Category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.score(training_vecs, training_data[\"Category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = pd.read_parquet('./testing.parquet')\n",
    "testing_vecs=feat_pipeline.transform(testing_data[\"Text\"])\n",
    "xgb.score(testing_vecs, testing_data[\"Category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlworkflows import plot\n",
    "\n",
    "df, chart =plot.confusion_matrix(testing_data[\"Category\"], xgb.predict(testing_vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(testing_data[\"Category\"], xgb.predict(testing_vecs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlworkflows import util\n",
    "\n",
    "util.serialize_to(xgb, \"model.sav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlworkflows import util\n",
    "\n",
    "util.serialize_to(xgb, \"model.sav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
